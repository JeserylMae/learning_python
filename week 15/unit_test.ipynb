{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_func(a, b):\n",
    "    \"\"\"This function takes two variables, adds them, and returns their sum.\n",
    "    \"\"\"\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This function takes two variables, adds them, and returns their sum.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(add_func.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_func(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_add_numbers():\n",
    "    try:\n",
    "        assert add_func(2, 3) == 5\n",
    "        print(\"Test Case 1 PASSED: add_func(2, 3) == 5\")\n",
    "    except AssertionError:\n",
    "        print(\"Test Case 1 FAILED: add_func(2, 3) returned an unexpected result\")\n",
    "        \n",
    "    try:\n",
    "        assert add_func(0, 0) == 0\n",
    "        print(\"Test Case 2 PASSED: add_func(0, 0) == 0\")\n",
    "    except AssertionError:\n",
    "        print(\"Test Case 2 FAILED: add_func(0, 0) returned an unexpected result\")\n",
    "        \n",
    "    try:\n",
    "        assert add_func(-1, 1) == 0\n",
    "        print(\"Test Case 3 PASSED: add_func(-1, 1) == 0\")\n",
    "    except AssertionError:\n",
    "        print(\"Test Case 3 FAILED: add_func(-1, 1) returned an unexpected result\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case 1 PASSED: add_func(2, 3) == 5\n",
      "Test Case 2 PASSED: add_func(0, 0) == 0\n",
      "Test Case 3 PASSED: add_func(-1, 1) == 0\n"
     ]
    }
   ],
   "source": [
    "test_add_numbers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Test 1 passed\n",
      "✓ Test 2 passed\n",
      "✓ Test 3 passed\n",
      "\n",
      "3 out of 3 tests passed (100.00%)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# my_module.py\n",
    "def add_numbers(a, b):\n",
    "    return a + b\n",
    "\n",
    "def test_add_numbers():\n",
    "    num_tests = 0  # initialize counter for total number of tests\n",
    "    num_passed = 0  # initialize counter for number of passing tests\n",
    "    try:\n",
    "        assert add_numbers(2, 3) == 5\n",
    "        print(u\"\\u2713 Test 1 passed\")\n",
    "        num_passed += 1  # increment passing tests counter\n",
    "    except AssertionError:\n",
    "        print(u\"\\u2717 Test 1 failed\")\n",
    "    num_tests += 1  # increment total tests counter\n",
    "    time.sleep(1)  # wait for 1 second\n",
    "    try:\n",
    "        assert add_numbers(0, 0) == 0\n",
    "        print(u\"\\u2713 Test 2 passed\")\n",
    "        num_passed += 1  # increment passing tests counter\n",
    "    except AssertionError:\n",
    "        print(u\"\\u2717 Test 2 failed\")\n",
    "    num_tests += 1  # increment total tests counter\n",
    "    time.sleep(1)  # wait for 1 second\n",
    "    try:\n",
    "        assert add_numbers(-1, 1) == 0\n",
    "        print(u\"\\u2713 Test 3 passed\")\n",
    "        num_passed += 1  # increment passing tests counter\n",
    "    except AssertionError:\n",
    "        print(u\"\\u2717 Test 3 failed\")\n",
    "    num_tests += 1  # increment total tests counter\n",
    "    time.sleep(1)  # wait for 1 second\n",
    "\n",
    "    # calculate and print percentage of passing tests\n",
    "    percent_passed = num_passed / num_tests * 100\n",
    "    print(f\"\\n{num_passed} out of {num_tests} tests passed ({percent_passed:.2f}%)\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_add_numbers()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Test 1 passed\n",
      "✓ Test 2 passed\n",
      "✓ Test 3 passed\n",
      "✓ Test 4 passed\n",
      "✓ Test 5 passed\n",
      "\n",
      "5 out of 5 tests passed (100.00%)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# my_module.py\n",
    "def add_numbers(a, b):\n",
    "    return a + b\n",
    "\n",
    "def test_add_numbers():\n",
    "    # list of test cases as tuples containing arguments and expected output\n",
    "    test_cases = [\n",
    "        (2, 3, 5),\n",
    "        (0, 0, 0),\n",
    "        (-1, 1, 0),\n",
    "        (100, -100, 0),\n",
    "        (3.5, 4.2, 7.7)\n",
    "    ]\n",
    "\n",
    "    num_tests = 0  # initialize counter for total number of tests\n",
    "    num_passed = 0  # initialize counter for number of passing tests\n",
    "\n",
    "    for i, (a, b, expected) in enumerate(test_cases):\n",
    "        num_tests += 1  # increment total tests counter\n",
    "        time.sleep(1)  # wait for 1 second\n",
    "        try:\n",
    "            assert add_numbers(a, b) == expected\n",
    "            print(u\"\\u2713 Test {} passed\".format(i+1))\n",
    "            num_passed += 1  # increment passing tests counter\n",
    "        except AssertionError:\n",
    "            print(u\"\\u2717 Test {} failed\".format(i+1))\n",
    "\n",
    "    # calculate and print percentage of passing tests\n",
    "    percent_passed = num_passed / num_tests * 100\n",
    "    print(f\"\\n{num_passed} out of {num_tests} tests passed ({percent_passed:.2f}%)\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_add_numbers()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Test 1 passed\n",
      "✓ Test 2 passed\n",
      "✓ Test 3 passed\n",
      "✓ Test 4 passed\n",
      "✓ Test 5 passed\n",
      "✓ Test 6 passed\n",
      "✓ Test 7 passed\n",
      "\n",
      "7 out of 7 tests passed (100.00%)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "test_cases = [\n",
    "        (2, 3, 5),\n",
    "        (0, 0, 0),\n",
    "        (-1, 1, 0),\n",
    "        (100, -100, 0),\n",
    "        (3.5, 4.2, 7.7), (55, 10, 65), (-80, -20, -100)]\n",
    "\n",
    "# my_module.py\n",
    "def add_numbers(a, b):\n",
    "    return a + b\n",
    "\n",
    "def test_add_numbers(test_cases=test_cases):\n",
    "    # list of test cases as tuples containing arguments and expected output\n",
    "    test_cases = test_cases\n",
    "\n",
    "    num_tests = len(test_cases)\n",
    "    num_passed = 0\n",
    "\n",
    "    for i, (a, b, expected) in enumerate(test_cases):\n",
    "        time.sleep(1)  # wait for 1 second\n",
    "        result = add_numbers(a, b)\n",
    "        if result == expected:\n",
    "            print(u\"\\u2713 Test {} passed\".format(i+1))\n",
    "            num_passed += 1\n",
    "        else:\n",
    "            print(u\"\\u2717 Test {} failed\".format(i+1))\n",
    "\n",
    "    percent_passed = num_passed / num_tests * 100\n",
    "    print(f\"\\n{num_passed} out of {num_tests} tests passed ({percent_passed:.2f}%)\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_add_numbers()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".....\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 0.005s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class MyTest(unittest.TestCase):\n",
    "    def test_addition(self):\n",
    "        self.assertEqual(1 + 1, 2)\n",
    "\n",
    "    def test_addition_negative_numbers(self):\n",
    "        self.assertEqual(-1 + (-1), -2)\n",
    "\n",
    "    def test_addition_zero(self):\n",
    "        self.assertEqual(0 + 0, 0)\n",
    "\n",
    "    def test_addition_large_numbers(self):\n",
    "        self.assertEqual(100000 + 100000, 200000)\n",
    "\n",
    "    def test_addition_floats(self):\n",
    "        self.assertAlmostEqual(1.5 + 2.5, 4.0, places=5)\n",
    "\n",
    "    def extra(self):\n",
    "        self.assertEqual(300 + 1, 301)\n",
    "\n",
    "# Create a test suite and add the test cases to it\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(MyTest)\n",
    "\n",
    "# Use the TextTestRunner to run the tests and output the results\n",
    "runner = unittest.TextTestRunner()\n",
    "result = runner.run(suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".......\n",
      "----------------------------------------------------------------------\n",
      "Ran 7 tests in 0.013s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import unittest\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "class bird:\n",
    "    def chirp(self):\n",
    "        print(\"The bird chirped!\")\n",
    "\n",
    "class BirdTestCase(unittest.TestCase):\n",
    "    def test_chirp(self):\n",
    "        b = bird()\n",
    "        with io.StringIO() as buf, redirect_stdout(buf):\n",
    "            b.chirp()\n",
    "            output = buf.getvalue().strip()\n",
    "        self.assertEqual(output, \"The bird chirped!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], exit=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bird:\n",
    "    def chirp(self):\n",
    "        return \"The bird chirped!\"\n",
    "    \n",
    "    def eat(self, food):\n",
    "        return f\"The bird ate {food}!\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class BirdTestCase(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.bird = Bird()\n",
    "        \n",
    "    def test_chirp(self):\n",
    "        self.assertEqual(self.bird.chirp(), \"The bird chirped!\")\n",
    "        \n",
    "    def test_eat(self):\n",
    "        self.assertEqual(self.bird.eat(\"seeds\"), \"The bird ate seeds!\")\n",
    "\n",
    "    def test_eat_apple(self):\n",
    "        self.assertEqual(self.bird.eat(\"apples\"), \"The bird ate apples!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_chirp (__main__.BirdTestCase) ... ok\n",
      "test_eat (__main__.BirdTestCase) ... ok\n",
      "test_eat_apple (__main__.BirdTestCase) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.007s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=3 errors=0 failures=0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suite = unittest.TestLoader().loadTestsFromTestCase(BirdTestCase)\n",
    "unittest.TextTestRunner(verbosity=2).run(suite)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projects2022",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
